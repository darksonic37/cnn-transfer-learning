\chapter{Conclusion}
\label{chapter:conclusion}

In this section we present conclusions, final remarks, and point to directions for future work.

\section{Discussion}

The following conclusions were made:

\begin{itemize}
    \item Transfer learning is not such a simple off-the-shelf technique. The network needs to be carefully explored, as was shown by the subpar results of the Inception V3 models;
    \item Extracting all features is likely the best idea. Optimization algorithms work globally, not in layers, so arbitrarily extracting features from layers is not ideal because neurons have developed co-dependencies between other neurons deep in the network;
    \item Sometimes extracting features from middle layers can be highly performant too because these features represent low-level concepts like shapes and lines which are more useful for a different target domain;
    \item Successful transfer learning depends largely on regularization techniques and an appropriate amount of data, otherwise models will be high variance.
\end{itemize}

\section{Future Work}

The VGG16 architecture, although very capable given its age, is not state-of-the-art anymore and attention has long shifted to residual networks. In such complex networks it is not clear how transfer learning can be applied effectively, and the kind of exhaustive study that was done for VGG16 is not feasible for such networks because the number of layers is in the hundreds which quickly originates a combinatorial explosion of all the possible configurations for extracting and freezing weights.

In that sense, it would be interesting to see a similar study for more recent architectures, which of course requires a very deep understanding of their inner workings, unlike VGG16 which is very straight forward in comparison.
