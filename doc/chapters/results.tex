\chapter{Results}
\label{chapter:results}

This chapter describes the experiments in terms of reproducible steps, settings, parameters, and conditions, as well as present and discuss their results.

Ultimately the experiments aim to study transfer learning in the specific domain of skin lesion classification, as well as compare its efficacy against simpler and more traditional learning schemes, by training and testing several models of different architectures and drawing helpful conclusions about the use of transfer learning techniques.

\section{Transfer Learning Experiments}

Based on the transfer learning techniques introduced, models of the VGG16 architecture pre-trained on ImageNet will be explored in order to repurpose the weights to a new model for skin lesion classification and draw conclusions about its efficacy.

It is hypothesized that extracting features from lower layers of models trained for ImageNet classification can provide good performance, because the former presumably provide low-level features (e.g. shapes or lines) that are still useful for skin lesion classification whereas the latter actually provide very high-level concepts (e.g. dogs, cats) that are relevant for classification in the ImageNet domain and need to be fine-tuned to the target dataset.

To verify this, experiments based on this transfer learning approach will adhere to the following common methodology:

\begin{enumerate}
    \item Standardize training and validation samples relative to ImageNet;
    \item Some parameters are transfered from pre-trained models and otherwise initialized according to Xavier initialization;
    \item Define network architecture:
        \begin{enumerate}
            \item Extract and freeze layers from the pre-trained model according to the specific experiment at hand;
            \item Use global average pooling to reduce the number of parameters before the classifier based on fully-connected layers;
            \item Use one fully-connected layer of 512 ReLU-activated neurons;
            \item Use one fully-connected layer of 1 sigmoid-activated neuron for binary classification.
        \end{enumerate}
    \item Mini-batch \ac{SGD} with momentum $\gamma = 0.9$:
        \begin{itemize}
            \item Binary cross entropy cost function and explicit L2 regularization;
            \item 32 samples batches;
            \item Initial learning rate $\eta = 10^{-4}$ that decays by a factor of $10$ if the validation accuracy has not improved $+10^{-3}$ in the last $10$ epochs;
            \item Shuffle the $m$ samples every epoch;
            \item Train for a maximum of 1000 epochs, stopping early if the loss has not changed $\pm 10^{-3}$ in the last $30$ epochs.
        \end{itemize}
    \item Grid-search model selection based on the accuracy as measured on a fixed validation set;
    \item Final models will be evaluated and compared primarily using accuracy as measured on the test set.
\end{enumerate}

The basic unit in traditional \ac{CNN} architectures like VGG16 is the convolutional block, which is typically comprised by a stack of convolutional layers and one pooling layer for downsampling. These convolutional blocks are then further stacked together to progressively output higher level feature maps at the end of each block.

To verify this, one can run a sample input (figure \ref{fig:sample_input}) through the original VGG16 model pre-trained on ImageNet and visualize the activations at the end of each block (after its pooling layer).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\textwidth]{figs/sample_input.jpg}
    \caption{A sample of the ISIC2018 train set showing a clear separation of background (skin) and foreground (lesion).}
    \label{fig:sample_input}
\end{figure}

As expected, the features detected in the early layers of the network (figure \ref{fig:vgg16_block1}) activate for low level concepts like shapes (the border around the lesion), background (the skin), and foreground (the lesion itself).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/vgg16_block1.png}
    \caption{The 64 feature maps at the end of block 1 of the VGG16 model exactly as trained on ImageNet.}
    \label{fig:vgg16_block1}
\end{figure}

On the other hand at the last block of the network the feature maps exhibit much higher level (and low dimensional) concepts (figure \ref{fig:vgg16_block5}) incomprehensible to the human eye.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/vgg16_block5.png}
    \caption{64 of the feature maps at the end of block 5 of the VGG16 model exactly as trained on ImageNet.}
    \label{fig:vgg16_block5}
\end{figure}

Clearly, successful transfer learning from the VGG16 pre-trained model is highly dependent on the layer up to which weights are extracted from. Presumably extracting all layers and fine-tuning them to the target dataset will always work, but in some cases it might be useful to extract only a subset of the features in order to produce a smaller model for applications where space (occupied by the model) and computation (required by the forward pass of an inherently deeper model) are at a premium (e.g. smartphones). Both of these cases (and more) will be explored.

Specifically, the \verb|tf.keras.applications.vgg16| implementation of the VGG16 model pre-trained on ImageNet organizes layers as such:

\begin{itemize}
    \item Layer 0 is the input layer (\verb|input_1|) and will not be considered for extraction as it merely represents the original input;
    \item Layers 1-3 are the first convolutional block (\verb|block1_conv1|, \verb|block1_conv2|, \verb|block1_pool|);
    \item Layers 4-6 are the second convolutional block (\verb|block2_conv1|, \verb|block2_conv2|, \verb|block2_pool|);
    \item Layers 7-10 are the third convolutional block (\verb|block3_conv1|, \verb|block3_conv2|, \verb|block3_conv3|, \verb|block3_pool|);
    \item Layers 11-14 are the fourth convolutional block (\verb|block4_conv1|, \verb|block4_conv2|, \verb|block4_conv3|, \verb|block4_pool|);
    \item Layers 15-18 are the fifth convolutional block (\verb|block5_conv1|, \verb|block5_conv2|, \verb|block5_conv3|, \verb|block5_pool|);
    \item Layers 19-22 are non-convolutional layers (\verb|flatten|, \verb|fc1|, \verb|fc2|, \verb|predictions|) and will not be considered for extraction.
\end{itemize}

With that in mind, in these experiments,

\begin{itemize}
    \item the number of the layer $e$ up to which parameters will be extracted from will be sampled from $e \in \{18,14,10,6,3\}$;
    \item the number of the layer $f$ up to which parameters will be frozen will be sampled from $f \in \{18,14,10,6,3,0\}$;
    \item $\lambda$ is the L2-regularization strength sampled from $\lambda \in [10^{-4}, 10^{-2}]$ spaced evenly on a log-scale;
\end{itemize}

\subsection{Total Feature Extraction without Fine Tuning}

Arguably the simplest case is to extract and freeze all layers (i.e. $e = f = 18$), feed them to the classifier, and effectively only train the classifier.

Surprisingly models are converging nicely and quickly (e.g. figure \ref{fig:vgg16_total_convergence}) and already give very good results overall (table \ref{table:vgg16_total}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/vgg16_total_convergence.png}
    \caption{Convergence of a model where $e = f = 18$ and $\lambda = 0.00135721$.}
    \label{fig:vgg16_total_convergence}
\end{figure}

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & AUC & Precision & Recall & F1-Score \\
\hline
18.0 & 18.0 & 0.0001 & 1.0 & 0.914 & 0.914 & 0.914 & 0.916 & 0.914 & 0.914 \\
18.0 & 18.0 & 0.000154 & 0.995 & 0.9 & 0.911 & 0.911 & 0.914 & 0.911 & 0.911 \\
18.0 & 18.0 & 0.000239 & 1.0 & 0.911 & 0.914 & 0.914 & 0.916 & 0.914 & 0.914 \\
18.0 & 18.0 & 0.000368 & 1.0 & 0.908 & 0.915 & 0.915 & 0.917 & 0.915 & 0.915 \\
18.0 & 18.0 & 0.000569 & 0.994 & 0.904 & 0.912 & 0.912 & 0.914 & 0.912 & 0.912 \\
18.0 & 18.0 & 0.000879 & 0.998 & 0.911 & 0.915 & 0.915 & 0.918 & 0.915 & 0.915 \\
18.0 & 18.0 & 0.00136 & 0.998 & 0.911 & 0.918 & 0.918 & 0.92 & 0.918 & 0.918 \\
18.0 & 18.0 & 0.0021 & 0.994 & 0.908 & 0.915 & 0.915 & 0.917 & 0.915 & 0.915 \\
18.0 & 18.0 & 0.00324 & 0.991 & 0.902 & 0.911 & 0.911 & 0.914 & 0.911 & 0.911 \\
18.0 & 18.0 & 0.005 & 0.975 & 0.893 & 0.906 & 0.906 & 0.909 & 0.906 & 0.905 \\
\hline
 & & & $0.995\pm0.00713$ & $0.906\pm0.0061$ & $0.913\pm0.00311$ & $0.913\pm0.00311$ & $0.916\pm0.00284$ & $0.913\pm0.00311$ & $0.913\pm0.00335$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$ and $f = 18$.}
\label{table:vgg16_total}
\end{table}

The validation curve of the L2-regularization strength $\lambda$ hyperparameter in figure \ref{fig:vgg16_total_lambda} shows these models are neither overfitting nor underfitting as $A_{val}$ is approximating $A_{train}$ well.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/vgg16_total_lambda.png}
    \caption{Plot of L2-regularization strength $\lambda$ against accuracy on the test set $A_{test}$ when $e = 18$ and $f = 18$.}
    \label{fig:vgg16_total_lambda}
\end{figure}

Presumably these surprisingly good results stem from an appropriate number $m$ of training samples (relative to the number of free parameters in the network) and proper data augmentation.

Indeed, by fixating $\lambda = 0.000879$ (for example) and varying the number of samples as fractions ${0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}$ of the total $m$ training samples it can be seen (in table \ref{table:vgg16_total_debug}) that performance increases as the number of samples increases.

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c| }
\hline
Fraction & $A_{train}$ & $A_{val}$ & $A_{test}$ & Precision & Recall & F1-Score \\
\hline
0.1 & 0.977 & 0.771 & 0.784 & 0.784 & 0.784 & 0.784 \\
0.2 & 0.991 & 0.803 & 0.82  & 0.821 & 0.82  & 0.82  \\
0.3 & 0.973 & 0.819 & 0.835 & 0.836 & 0.835 & 0.835 \\
0.4 & 0.984 & 0.843 & 0.854 & 0.857 & 0.854 & 0.854 \\
0.5 & 0.997 & 0.863 & 0.872 & 0.872 & 0.872 & 0.872 \\
0.6 & 0.987 & 0.868 & 0.879 & 0.881 & 0.879 & 0.879 \\
0.7 & 0.996 & 0.881 & 0.889 & 0.891 & 0.889 & 0.889 \\
0.8 & 0.998 & 0.899 & 0.906 & 0.908 & 0.906 & 0.906 \\
0.9 & 0.989 & 0.894 & 0.904 & 0.906 & 0.904 & 0.904 \\
\hline
 & $0.988\pm0.00829$ & $0.849\pm0.0412$ & $0.86\pm0.0386$ & $0.862\pm0.0392$ & $0.86\pm0.0386$ & $0.86\pm0.0386$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$, $f = 18$, $\lambda = 0.000879$ and the fraction of the number of samples $m$ is varied.}
\label{table:vgg16_total_debug}
\end{table}

\subsection{Partial Feature Extraction without Fine Tuning}

Extracting a smaller number of layers seemed promising since that would yield a smaller and faster model. Besides, presumably, the most relevant features were in the middle layers anyway. However none of the models seemed to be converging usefully, likely stuck in a local minimum with high loss (e.g. figure \ref{fig:vgg16_partial_divergence}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/vgg16_partial_divergence.png}
    \caption{Partial feature extraction network (with $e = 14$, $f = 14$, $\lambda = 0.00015445$, $\eta = 10^{-4}$) not converging, stuck in a local minimum.}
    \label{fig:vgg16_partial_divergence}
\end{figure}

Accordingly, performance results (in table \ref{table:vgg16_partial}) were disappointing for any number $e < 18$ of frozen layers.

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & AUC & Precision & Recall & F1-Score \\
\hline
14.0 & 0.0 & 0.0001 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
14.0 & 0.0 & 0.000154 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
14.0 & 0.0 & 0.000239 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
14.0 & 0.0 & 0.000368 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
14.0 & 0.0 & 0.000569 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
3.0 & 3.0 & 0.000154 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.000239 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
3.0 & 3.0 & 0.000368 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.000569 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.000879 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.00136 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.0021 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.00324 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
3.0 & 3.0 & 0.005 & 0.5 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
\hline
 & & & $0.5\pm0.0$ & $0.5\pm0.0$ & $0.5\pm0.0$ & $0.5\pm0.0$ & $0.25\pm0.0$ & $0.5\pm0.0$ & $0.334\pm0.000497$ \\
\hline
\end{tabular}
\caption{Performance metrics for models where $e < 18$.}
\label{table:vgg16_partial}
\end{table}

It was hypothesized that the learning rate $\eta = 10^{-4}$ was adequate for the higher layers of the fifth convolutional block, but too high for the remaining layers where \ac{SGD} was overshooting the update of the parameters. Intuitively, the high learning rate was disrupting the parameters learned by the pre-trained model.

By fixating $e = 14$, $f = 14$, $\lambda = 0.00015445$, and cross-validating 20 new smaller values for the learning rate (from $\eta \in [10^0, 10^{-10}]$ spaced evenly on a log-scale) a new value $\eta = 0.000206913808$ (among others) was found to provide convergence (figure \ref{fig:vgg16_partial_convergence}) to very good performance ($A_{test} = 0.916$) while only extracting $e = 14$ layers which makes for a smaller, faster model.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/vgg16_partial_convergence.png}
    \caption{Partial feature extraction network (with $e = 14$, $f = 14$, $\lambda = 0.00015445$, $\eta = 10^{-4}$) converging to very good performance and modest model size.}
    \label{fig:vgg16_partial_convergence}
\end{figure}

A more thorough systematic study of the learning rate when extracting lower-level layers would certainly unveil more models with a good balance between model size and model performance, perhaps even outperforming models that extract all convolutional layers.

\subsection{Total Feature Extraction with Fine Tuning}

Presumably, further fine-tuning layers while extracting all $e = 18$ layers will yield higher performance since it will continue to optimize parameters relative to the target dataset thus minimizing error on said dataset.

Indeed, freezing the first $f = 14$ layers (versus the previous $e = f = 18$ configuration) yields a 2\% increase in accuracy on the test set (table \ref{table:vgg16_finetuning_14}).

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & Precision & Recall & F1-Score \\
\hline
18 & 14 & 0.0001 & 1.0 & 0.928 & 0.934 & 0.936 & 0.934 & 0.934 \\
18 & 14 & 0.000154 & 1.0 & 0.923 & 0.94 & 0.941 & 0.94 & 0.94 \\
18 & 14 & 0.000239 & 1.0 & 0.925 & 0.939 & 0.94 & 0.939 & 0.939 \\
18 & 14 & 0.000368 & 1.0 & 0.926 & 0.939 & 0.94 & 0.939 & 0.939 \\
18 & 14 & 0.000569 & 1.0 & 0.925 & 0.941 & 0.942 & 0.941 & 0.941 \\
18 & 14 & 0.000879 & 1.0 & 0.921 & 0.929 & 0.93 & 0.929 & 0.929 \\
18 & 14 & 0.00136 & 1.0 & 0.923 & 0.938 & 0.94 & 0.938 & 0.938 \\
18 & 14 & 0.0021 & 1.0 & 0.927 & 0.936 & 0.937 & 0.936 & 0.936 \\
18 & 14 & 0.00324 & 1.0 & 0.925 & 0.936 & 0.937 & 0.936 & 0.936 \\
18 & 14 & 0.005 & 1.0 & 0.925 & 0.936 & 0.937 & 0.936 & 0.936 \\
\hline
 & & & $1.0\pm0.0$ & $0.925\pm0.00194$ & $0.937\pm0.00331$ & $0.938\pm0.00329$ & $0.937\pm0.00331$ & $0.937\pm0.00331$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$ and $f = 14$.}
\label{table:vgg16_finetuning_14}
\end{table}

However, as $f$ decreases (to take values $f = 10$, $f = 6$, $f = 3$, and $f = 0$) the trend seems to be that performance declines (tables \ref{table:vgg16_finetuning_10}, \ref{table:vgg16_finetuning_6}, \ref{table:vgg16_finetuning_3}, and \ref{table:vgg16_finetuning_0} respectively).

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & Precision & Recall & F1-Score \\
\hline
18 & 10 & 0.0001 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
18 & 10 & 0.000154 & 1.0 & 0.926 & 0.934 & 0.935 & 0.934 & 0.934 \\
18 & 10 & 0.000239 & 1.0 & 0.928 & 0.931 & 0.933 & 0.931 & 0.931 \\
18 & 10 & 0.000368 & 1.0 & 0.922 & 0.931 & 0.932 & 0.931 & 0.931 \\
18 & 10 & 0.000569 & 1.0 & 0.933 & 0.943 & 0.945 & 0.943 & 0.943 \\
18 & 10 & 0.000879 & 1.0 & 0.925 & 0.93 & 0.931 & 0.93 & 0.93 \\
18 & 10 & 0.00136 & 1.0 & 0.935 & 0.937 & 0.938 & 0.937 & 0.937 \\
18 & 10 & 0.0021 & 1.0 & 0.931 & 0.927 & 0.928 & 0.927 & 0.927 \\
18 & 10 & 0.00324 & 1.0 & 0.929 & 0.94 & 0.941 & 0.94 & 0.94 \\
18 & 10 & 0.005 & 1.0 & 0.929 & 0.932 & 0.933 & 0.932 & 0.932 \\
\hline
 & & & $0.95\pm0.15$ & $0.886\pm0.129$ & $0.891\pm0.13$ & $0.867\pm0.206$ & $0.891\pm0.13$ & $0.874\pm0.18$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$ and $f = 10$.}
\label{table:vgg16_finetuning_10}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & Precision & Recall & F1-Score \\
\hline
18 & 6 & 0.0001 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 6 & 0.000154 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 6 & 0.000239 & 1.0 & 0.93 & 0.927 & 0.928 & 0.927 & 0.927 \\
18 & 6 & 0.000368 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 6 & 0.000569 & 1.0 & 0.925 & 0.927 & 0.928 & 0.927 & 0.927 \\
18 & 6 & 0.000879 & 1.0 & 0.922 & 0.928 & 0.929 & 0.928 & 0.928 \\
18 & 6 & 0.00136 & 0.5 & 0.501 & 0.499 & 0.416 & 0.499 & 0.334 \\
18 & 6 & 0.0021 & 0.501 & 0.498 & 0.5 & 0.5 & 0.5 & 0.336 \\
18 & 6 & 0.00324 & 1.0 & 0.917 & 0.916 & 0.917 & 0.916 & 0.916 \\
18 & 6 & 0.005 & 1.0 & 0.922 & 0.926 & 0.927 & 0.926 & 0.926 \\
\hline
 & & & $0.75\pm0.25$ & $0.711\pm0.212$ & $0.712\pm0.213$ & $0.629\pm0.306$ & $0.712\pm0.213$ & $0.629\pm0.296$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$ and $f = 6$.}
\label{table:vgg16_finetuning_6}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & Precision & Recall & F1-Score \\
\hline
18 & 3 & 0.0001 & 1.0 & 0.9 & 0.913 & 0.913 & 0.913 & 0.913 \\
18 & 3 & 0.000154 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 3 & 0.000239 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 3 & 0.000368 & 1.0 & 0.915 & 0.924 & 0.925 & 0.924 & 0.924 \\
18 & 3 & 0.000569 & 1.0 & 0.926 & 0.931 & 0.932 & 0.931 & 0.931 \\
18 & 3 & 0.000879 & 1.0 & 0.927 & 0.933 & 0.934 & 0.933 & 0.933 \\
18 & 3 & 0.00136 & 1.0 & 0.933 & 0.931 & 0.933 & 0.931 & 0.931 \\
18 & 3 & 0.0021 & 1.0 & 0.931 & 0.932 & 0.933 & 0.932 & 0.932 \\
18 & 3 & 0.00324 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.334 \\
18 & 3 & 0.005 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
\hline
 & & & $0.8\pm0.245$ & $0.753\pm0.207$ & $0.756\pm0.209$ & $0.657\pm0.332$ & $0.756\pm0.209$ & $0.69\pm0.291$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$ and $f = 3$.}
\label{table:vgg16_finetuning_3}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }
\hline
$e$ & $f$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ & Precision & Recall & F1-Score \\
\hline
18 & 0 & 0.0001 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 0 & 0.000154 & 1.0 & 0.916 & 0.923 & 0.923 & 0.923 & 0.923 \\
18 & 0 & 0.000239 & 1.0 & 0.9 & 0.906 & 0.907 & 0.906 & 0.906 \\
18 & 0 & 0.000368 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 0 & 0.000569 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 0 & 0.000879 & 0.501 & 0.5 & 0.499 & 0.25 & 0.499 & 0.333 \\
18 & 0 & 0.00136 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 0 & 0.0021 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 0 & 0.00324 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
18 & 0 & 0.005 & 0.5 & 0.5 & 0.5 & 0.25 & 0.5 & 0.333 \\
\hline
 & & & $0.6\pm0.2$ & $0.582\pm0.163$ & $0.583\pm0.166$ & $0.383\pm0.266$ & $0.583\pm0.166$ & $0.449\pm0.233$ \\
\hline
\end{tabular}
\caption{Performance metrics of models when $e = 18$ and $f = 0$.}
\label{table:vgg16_finetuning_0}
\end{table}

This can be explained by an inadequate setting of the learning rate just as in the previous set of experiments. The more higher layers are unfrozen, the more their weights are disrupted too quickly because of too high a learning rate.

%\subsection{Gradual Unfreezing}

%Lastly, a different transfer learning scheme will be evaluated where $e$ layers are initially extracted but all are frozen and only the classifier is trained at first. Then layers are progressively unfrozen from right to left and trained with an appropriately set learning rate (and early stopping patience) for each new set of unfrozen layers.

%When extracting $e = 18$ layers

%\begin{itemize}
%    \item first the network will be trained with $f = 18$ (i.e. only the classifier will be trained), learning rate $\eta = 10^{-3}$, and early stopping patience $p = 30$.
%\end{itemize}

\section{End-to-End Learning Experiments}

This section describes a set of experiments involving custom designed \ac{CNN} architectures whose models are trained from scratch, following the more traditional methodology typically called end-to-end learning, that will be used for comparison against the transfer learning approach which it is in direct contrast with.

These custom architectures are based around reasonable heuristics\footnote{\url{https://cs231n.github.io/convolutional-networks/}}:

% TODO: rewrite words
\begin{itemize}
    \item The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers.
    \item Prefer a stack of small filter convolutional layers to one large receptive field convolutional layer
    \item It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.
    \item Reducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and symmetrically wired.
    \item Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.
    \item Why use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly.
\end{itemize}

The experiments models of such architectures were trained from scratch as follows:

\begin{enumerate}
    \item Standardize training and validation samples relative to \ac{ISIC} 2018;
    \item Parameters are all initialized according to Xavier initialization;
    \item Define network architecture:
        \begin{enumerate}
            \item Stack blocks of convolutional and pooling layers to build useful features for classification;
            \item Use global average pooling to reduce the number of parameters before the classifier based on fully-connected layers;
            \item Stack fully-connected layers of ReLU-activated neurons;
            \item Add fully-connected layer with a single sigmoid-activated neuron for binary classification.
        \end{enumerate}
    \item Mini-batch \ac{SGD} with momentum $\gamma = 0.9$:
        \begin{itemize}
            \item Binary cross entropy cost function and explicit L2 regularization;
            \item 32 samples batches;
            \item Shuffle the $m$ samples every epoch;
            \item Initial learning rate $\eta = 10^{-4}$ that decays by a factor of $10$ if the validation accuracy has not improved $+10^{-3}$ in the last $10$ epochs;
            \item Train for a maximum of 1000 epochs, stopping early if the loss has not changed $\pm 10^{-3}$ in the last $30$ epochs.
        \end{itemize}
    \item Grid-search model selection based on the accuracy as measured on a fixed validation set;
    \item Final models will be evaluated and compared primarily using accuracy as measured on the test set.
\end{enumerate}

\subsection{Custom Architecture 1}

This custom architecture, illustrated in figure \ref{fig:custom1}, stacks three convolution-pooling blocks. Each block stacks two convolutional layers before the pooling layer, which is a good idea for large and deep networks because multiple stacked convolutional layers can develop more complex features of the input volume before the destructive pooling operation. Specifically,

% TODO: consistency
\begin{enumerate}
    \item $K = 32$ filters, $F = 3$ kernel, $S = 1$ stride, same padding, ReLU activated
    \item 32 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
    \item 2x2 max pooling
    \item 64 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
    \item 64 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
    \item 2x2 max pooling
    \item 128 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
    \item 128 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
\end{enumerate}

The classifier, illustrated in figure \ref{fig:custom1}, is a stack of two fully-connected layers of ReLU-activated neurons followed by a fully-connected sigmoid-activated neuron for binary classification.

\begin{enumerate}
    \item $u$ fully-connected ReLU-activated neurons;
    \item Another $u$ fully-connected ReLU-activated neurons;
    \item Single fully-connected sigmoid-activated neuron for binary classification.
\end{enumerate}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/custom1.png}
    \caption{Architecture of the first custom-designed neural network.}
    \label{fig:custom1}
\end{figure}

We grid-search the best $\lambda$ for L2-regularization from $\lambda \in \{10^{-10}, ..., 10^{2}\}$ (spaced evenly on a log scale) as well as the number $u$ of ReLU-activated neurons in the antepenultimate and penultimate fully-connected layers from $u \in \{ 64, 128, 256, 512, 1024, 2048 \}$. This yields a total of $60$ models, of which the top 20 are summarized in table \ref{table:top20_custom1}.

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c| }
\hline
$u$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ \\
\hline
128 & 1.61e-44 & 0.725 & 0.722 & 0.728 \\
256 & 1.27e-17 & 0.722 & 0.721 & 0.728 \\
256 & 3.29e-05 & 0.722 & 0.722 & 0.728 \\
256 & 1.74e-13 & 0.723 & 0.721 & 0.728 \\
256 & 7.88e-24 & 0.722 & 0.721 & 0.728 \\
64 & 3.04e-36 & 0.722 & 0.723 & 0.728 \\
256 & 1.17e-48 & 0.721 & 0.719 & 0.727 \\
256 & 2.21e-40 & 0.723 & 0.721 & 0.727 \\
256 & 2.4e-09 & 0.722 & 0.721 & 0.727 \\
64 & 2.59e-38 & 0.723 & 0.722 & 0.727 \\
64 & 1.61e-44 & 0.723 & 0.723 & 0.727 \\
64 & 1.89e-42 & 0.722 & 0.724 & 0.727 \\
64 & 6.72e-26 & 0.722 & 0.724 & 0.727 \\
128 & 1.08e-19 & 0.725 & 0.722 & 0.727 \\
256 & 1.08e-19 & 0.722 & 0.72 & 0.727 \\
256 & 1e-50 & 0.722 & 0.719 & 0.727 \\
256 & 9.24e-22 & 0.722 & 0.721 & 0.727 \\
64 & 1.27e-17 & 0.722 & 0.722 & 0.727 \\
64 & 2.21e-40 & 0.722 & 0.72 & 0.727 \\
64 & 3.29e-05 & 0.721 & 0.723 & 0.727 \\
\hline
\end{tabular}
\caption{Top 20 models of the custom architecture 1 sorted by $A_{test}$ in descending order.}
\label{table:top20_custom1}
\end{table}

The effect of $\lambda$ and $u$ on the performance of the model can be understood in figure \ref{fig:lambda_units_study_custom1}. As $u$ gets larger performance gets subtly but increasingly lower, so large $u$ is not worth it, especially when you consider the consequently larger number of parameters in the fully-connected layers which would have to be trained.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../plots/plots_7/lambda_units_study/lambda_units_study.png}
    \caption{Accuracy of custom architecture 1 over a range of values for L2-regularization strength and the number of ReLU-activated neurons in the penultimate fully-connected layer.}
    \label{fig:lambda_units_study_custom1}
\end{figure}

\subsection{Custom Architecture 2}

This custom architecture, illustrated in figure \ref{fig:custom2}, stacks two convolution-pooling blocks. Each block uses a single convolutional layer before the pooling layer.

\begin{enumerate}
    \item 32 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
    \item 2x2 max pooling
    \item 64 filters, 3x3 kernel, 1x1 stride, same padding, ReLU activated
    \item 2x2 max pooling
\end{enumerate}

The classifier is a single fully-connected layer of $u$ ReLU-activated neurons followed by a fully-connected sigmoid-activated neuron for binary classification.

\begin{enumerate}
    \item $u$ fully-connected ReLU-activated neurons;
    \item Single fully-connected sigmoid-activated neuron for binary classification.
\end{enumerate}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/custom2.png}
    \caption{Custom Architecture 2}
    \label{fig:custom2}
\end{figure}

We grid-search the best $\lambda$ for L2-regularization from $\lambda \in \{10^{-10}, ..., 10^{2}\}$ (spaced evenly on a log scale) as well as the number $u$ of ReLU-activated neurons in the penultimate fully-connected layer from $u \in \{ 64, 128, 256, 512, 1024, 2048 \}$. This yields a total of $60$ models, of which the top 20 are summarized in table \ref{table:top20_custom2}.

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c| }
\hline
$u$ & $\lambda$ & $A_{train}$ & $A_{val}$ & $A_{test}$ \\
\hline
64 & 1e-50 & 0.722 & 0.722 & 0.737 \\
64 & 0.452 & 0.656 & 0.657 & 0.651 \\
256 & 85300000.0 & 0.621 & 0.613 & 0.612 \\
512 & 9.24e-22 & 0.587 & 0.583 & 0.584 \\
256 & 1.74e-13 & 0.574 & 0.577 & 0.575 \\
512 & 53.0 & 0.567 & 0.58 & 0.558 \\
256 & 7.88e-24 & 0.56 & 0.544 & 0.553 \\
128 & 728000.0 & 0.556 & 0.559 & 0.547 \\
256 & 10000000000 & 0.544 & 0.54 & 0.534 \\
512 & 1.74e-13 & 0.537 & 0.522 & 0.534 \\
512 & 0.452 & 0.512 & 0.517 & 0.531 \\
128 & 3.04e-36 & 0.542 & 0.536 & 0.526 \\
64 & 2.81e-07 & 0.51 & 0.507 & 0.514 \\
1024 & 7.88e-24 & 0.496 & 0.5 & 0.504 \\
2048 & 2.04e-11 & 0.502 & 0.5 & 0.501 \\
1024 & 0.00386 & 0.5 & 0.5 & 0.5 \\
1024 & 1.49e-15 & 0.5 & 0.5 & 0.5 \\
1024 & 1e-50 & 0.5 & 0.5 & 0.5 \\
1024 & 2.4e-09 & 0.5 & 0.5 & 0.5 \\
1024 & 2.59e-38 & 0.5 & 0.5 & 0.5 \\
\hline
\end{tabular}
\caption{Top 20 models of the custom architecture 2 sorted by $A_{test}$ in descending order.}
\label{table:top20_custom2}
\end{table}

% TODO: citation needed

The effect of $\lambda$ and $u$ on the performance of the model can be understood in figure \ref{fig:lambda_units_study_custom2}. Since models of this architecture have much less parameters they are much less prone to overfitting, meaning that a wider range of values of $\lambda$ result in good performance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../plots/plots_2/lambda_units_study/lambda_units_study.png}
    \caption{Accuracy of custom architecture 2 over a range of values for L2-regularization strength.}
    \label{fig:lambda_units_study_custom2}
\end{figure}
