\chapter{Artificial Neural Networks}
\label{chapter:ann}

In this section we go over artificial neural networks.

\section{Supervised Learning}

A supervised learning problem is one where we have $m$ labeled training samples $(x^{(i)}, y^{(i)})$ where $x^{(i)} \in \mathbb{R}^n$ and $y^{(i)} \in {0, 1}$.

\ac{ANN} are most used in supervised learning, but can also be used in unsupervised learning problems.

\section{Classification}

In a classification problem, $J \colon \mathbb{R}^n \to \mathbb{R}$.

\section{Cost Function}

Let $L \colon \mathbb{R}^n \to \mathbb{R}$ be a function, called the loss function, which quantifies the quality of a particular set of parameters $\theta$ relative to a single data sample.

Let $J \colon \mathbb{R}^n \to \mathbb{R}$ be a function, called the cost function, which quantifies the quality of a particular set of parameters $\theta$ relative to all the data samples in the training data.

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} L(x^{(i)}, y^{(i)}, \theta)
$$

\section{Optimization}

Given a cost function $J$, the goal is to find the optimal $\theta^*$ that minimizes the cost function, i.e. the optimization objective is 

$$
\theta^* = \argmin_{\theta} J(\theta)
$$

In the deep learning high-dimensional optimization landscape, the objective function is non-convex w.r.t. the parameters, so one can't use any of the tools from the convex optimization literature. Using calculus to find the minimum analytically is only feasible for trivial, low-dimensional, convex functions.

\subsection{Brute-force Search}

A naive first approach is to systematically and exhaustively enumerate and evaluate many candidate solutions while keeping track of the best one. This simple solution, arguably the simplest metaheuristic, is infeasible in the context of deep neural networks due to the curse of dimensionality.

\subsection{Random Optimization}

Rather than try all solutions exhaustively, another approach is to try random values of $\theta$ in a loop and record the running best until some stopping criteria.

\subsection{Random Local Search}

A slightly better strategy is to start with random $\theta$ and iteratively refine the best-found solution (hence the local in local search) until some stopping criteria.

\subsection{Gradient Descent}

Random local search forms a good basis for optimization, but there is no need to move randomly in search-space. The negative of the gradient of the cost function $\nabla_{\theta} J(\theta)$ w.r.t. the parameters $\theta$ over all $m$ training samples gives the direction of steepest descent (i.e. the direction in which the cost function decreases):

$$
\nabla_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \nabla_{\theta} L(x^{(i)}, y^{(i)}, \theta)
$$

A parameter $\eta \in \mathbb{{R}^{+}}$ (called the learning rate) controls the size of the step to take in the direction of the gradient. Thus emerges a very simple and natural update rule for our parameters:

$$
\theta^{t+1} = \theta^t - \eta \nabla_{\theta} J(\theta)
$$

% TODO: needs a bit more
The learning rate $\eta$ must be set carefully to ensure convergence.

\subsection{Mini-Batch Gradient Descent}

Computing the gradient $\nabla J$ with respect to all the $m$ training samples is computationally intensive. Instead $\nabla J_x$ for $m'$ randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient $\nabla C$, and this helps speed up gradient descent, and thus learning.

To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number $m$ of randomly chosen training inputs. 

\begin{eqnarray}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\end{eqnarray}

% SGD is faster approximation

\subsection{Stochastic Gradient Descent}

Stochastic gradient descent can be seen as a special case of mini-batch gradient descent when the batch size is 1.



% TODO: Explain variations of GD
% http://ruder.io/optimizing-gradient-descent/

Gradient descent can be further optimized \cite{ruder2016}.

\section{Comparison between gradient descent techniques}

% https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data

The key advantage of using minibatch as opposed to the full dataset goes back to the fundamental idea of stochastic gradient descent1.

In batch gradient descent, you compute the gradient over the entire dataset, averaging over potentially a vast amount of information. It takes lots of memory to do that. But the real handicap is the batch gradient trajectory land you in a bad spot (saddle point).

In pure SGD, on the other hand, you update your parameters by adding (minus sign) the gradient computed on a single instance of the dataset. Since it's based on one random data point, it's very noisy and may go off in a direction far from the batch gradient. However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minima(Theorem 6 in [2]). The disadvantage is it's terribly inefficient and you need to loop over the entire dataset many times to find a good solution.

The minibatch methodology is a compromise that injects enough noise to each gradient update, while achieving a relative speedy convergence.

%[1] Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.
%[2] Ge, R., Huang, F., Jin, C., & Yuan, Y. (2015, June). Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition. In COLT (pp. 797-842).

\section{Cross Validation}

% https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/

Often we want to tune the parameters of a model (for example, C in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation:

1. Set the parameter you want to tune to some value.
2. Split your data into K ‘folds’ (sections).
3. Train your model using K-1 folds using the parameter value.
4. Test your model on the remaining fold.
5. Repeat steps 3 and 4 so that every fold is the test data once.
6. Repeat steps 1 to 5 for every possible value of the parameter.
7. Report the parameter that produced the best result.

However, as Cawley and Talbot point out in their 2010 paper, since we used the test set to both select the values of the parameter and evaluate the model, we risk optimistically biasing our model evaluations. For this reason, if a test set is used to select model parameters, then we need a different test set to get an unbiased evaluation of that selected model.

One way to overcome this problem is to have nested cross validations. First, an inner cross validation is used to tune the parameters and select the best model. Second, an outer cross validation is used to evaluate the model selected by the inner cross validation.



\section{Neuron}

% TODO: definitely needs figure here, for illustration
A neuron is a computational unit parameterized by a weight matrix $W$ and bias vector $b$ which takes as input $x \in \mathbb{R}^{n}$ and outputs a hypothesis $h_{W,b}(x) = f(\sum^{i=1}_{n} W_{i}x_{i} + b) = f(W^Tx + b)$.

An activation function $f \colon \mathbb{R} \to \mathbb{R}$ must be differentiable for backpropagation to work. Sigmoid function is any bounded, differentiable, real function (e.g. logistic, tanh).

% TODO: table of common activation functions
The activation function $f$ is what characterizes the neuron.

% Explain why weights and biases matrix is declared the way it is (i.e. to take advantage of fast matrix math).
By the way, it's this expression that motivates the quirk in the wljk notation mentioned earlier. If we used j to index the input neuron, and k to index the output neuron, then we'd need to replace the weight matrix in Equation (25) by the transpose of the weight matrix. That's a small change, but annoying, and we'd lose the easy simplicity of saying (and thinking) "apply the weight matrix to the activations".


\subsubsection{Perceptron}

If the activation function $f$ is the step function, the neuron is reduced to the perceptron presented in 1950-1960 by Rosenblatt [x]. However the step function is shit because

Perceptron is the simplest neural network for classification of patterns that are linearly separable (which it is limited to).



If patterns are linearly separable then perecptron learning algorithm converges and represents a decision hyperplane separating the two classes

Patterns must be linearly separable in order to achieve accurate classification

%--------

We can see that there are two classification regions separated by a hyperplane in $m$-dimensional space:

$$
\sum_{j=1}^{m} w_j x_j + \theta = 0
$$

Neuron bias can be shown to be an additional input with a fixed value $1$ and weight $\theta$.

Let $p+1$-dimensional input vector be:

\begin{align}
x &= \begin{bmatrix}
    1 \\
    x_{1} \\
    x_{2} \\
    \vdots \\
    x_{p}
    \end{bmatrix}
\end{align}

Let $p+1$-dimensional weight vector be:

\begin{align}
w &= \begin{bmatrix}
    \theta \\
    w_{1} \\
    w_{2} \\
    \vdots \\
    w_{p}
    \end{bmatrix}
\end{align}

The internal neuron activity $z$ is the dot product of the weight vector $w$ and input vector $x$:

$$
z = w^T \cdot x
$$

$w^T \cdot x = 0$ defines a hyperplane in $p$-dimensional space of coordinates $x_1$, $x_2$, ..., $x_p$. If two pattern classes are linearly separable there exists weight vector $w$ so that:

\begin{itemize}
    \item $w^T \cdot x \geq 0$ for each $x$ belonging to class $C_1$
    \item $w^T \cdot x < 0$ for each $x$ belonging to $C_2$
\end{itemize}

The learning problem is to find a vector $w$ that provides such correct classification

\section{ReLU}
Lorem ipsum
